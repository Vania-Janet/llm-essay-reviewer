# Agente Evaluador de Ensayos ğŸ“

Sistema de evaluaciÃ³n automÃ¡tica de ensayos usando **LangGraph** y **LangChain** con GPT-4.

## âœ¨ CaracterÃ­sticas Principales

- âœ… **EvaluaciÃ³n automatizada** con 5 criterios acadÃ©micos rigurosos
- ğŸ“„ **Procesamiento de PDFs** con extracciÃ³n y limpieza inteligente
- ğŸ§¹ **Limpieza de texto con LLM** para PDFs mal formateados
- ğŸ“Š **Reportes HTML** detallados y visualmente atractivos
- ğŸ”„ **Procesamiento por lotes** de mÃºltiples ensayos
- ğŸ¯ **Structured output** para calificaciones precisas

## ğŸ“‹ Criterios de EvaluaciÃ³n

Este agente evalÃºa ensayos acadÃ©micos segÃºn 5 criterios especÃ­ficos con ponderaciones establecidas:

1. **Calidad tÃ©cnica y rigor acadÃ©mico (20%)** - Estructura, coherencia y solidez argumentativa
2. **Creatividad y originalidad (20%)** - Ideas nuevas y enfoques innovadores
3. **VinculaciÃ³n con ejes temÃ¡ticos (20%)** - TecnologÃ­a, sostenibilidad, inclusiÃ³n
4. **Bienestar colectivo y responsabilidad social (20%)** - Impactos sociales, Ã©ticos y ambientales
5. **Potencial de impacto y publicaciÃ³n (20%)** - Capacidad de comunicar e inspirar

Cada criterio se evalÃºa en una escala del 1 al 5 con comentarios detallados.

## ğŸ—ï¸ Arquitectura

El sistema utiliza **LangGraph** para crear un grafo de evaluaciÃ³n secuencial:

```
Inicio â†’ Calidad TÃ©cnica â†’ Creatividad â†’ VinculaciÃ³n â†’ Bienestar â†’ Impacto â†’ Comentario General â†’ Fin
```

Cada nodo del grafo:
- EvalÃºa un criterio especÃ­fico usando prompts especializados
- Asigna una calificaciÃ³n (1-5)
- Genera comentarios detallados
- Pasa el estado al siguiente nodo

## ğŸ“ Estructura del Proyecto

```
essay-agent/
â”œâ”€â”€ .env                    # Variables de entorno (OPENAI_API_KEY)
â”œâ”€â”€ requirements.txt        # Dependencias del proyecto
â”œâ”€â”€ models.py              # Modelos Pydantic para datos
â”œâ”€â”€ prompts.py             # Prompts del sistema
â”œâ”€â”€ agent.py               # Agente evaluador con LangGraph
â”œâ”€â”€ pdf_processor.py       # Procesador de PDFs con limpieza LLM
â”œâ”€â”€ main.py                # Script para evaluar ensayos .txt
â”œâ”€â”€ evaluar_batch.py       # EvaluaciÃ³n masiva de archivos .txt
â”œâ”€â”€ evaluar_pdfs.py        # EvaluaciÃ³n directa desde PDFs
â””â”€â”€ README.md              # Este archivo
```

## ğŸš€ InstalaciÃ³n

1. **Clonar o descargar el proyecto**

```bash
git clone https://github.com/Vania-Janet/llm-essay-reviewer.git
cd llm-essay-reviewer
```

2. **Instalar dependencias**:

```bash
pip install -r requirements.txt
```

**Nota**: Esto instalarÃ¡ automÃ¡ticamente:
- `langchain`, `langgraph`, `langchain-openai` (evaluaciÃ³n con LLMs)
- `pypdf` y `pdfplumber` (procesamiento de PDFs)
- `pydantic`, `python-dotenv` (utilidades)

3. **Configurar variables de entorno**:

Crea o edita el archivo `.env`:
```env
OPENAI_API_KEY=tu_clave_de_openai_aqui
```

**Obtener API key**: https://platform.openai.com/api-keys

4. **Verificar instalaciÃ³n**:

```bash
python test_pdf_processor.py
```

## ğŸ’» Uso

### OpciÃ³n 1: Evaluar ensayos desde PDFs (Recomendado)

```bash
python evaluar_pdfs.py
```

Este script:
1. Extrae texto del PDF usando pypdf o pdfplumber
2. Limpia el texto con LLM (quita nÃºmeros de pÃ¡gina, une lÃ­neas cortadas, etc.)
3. EvalÃºa el ensayo con los 5 criterios
4. Genera reportes HTML detallados

**Ejemplo de uso programÃ¡tico:**
```python
from evaluar_pdfs import evaluar_pdf

# Evaluar un PDF individual
evaluacion = evaluar_pdf("mi_ensayo.pdf", output_dir="reportes")

# Evaluar todos los PDFs de un directorio
from evaluar_pdfs import evaluar_directorio_pdfs
evaluar_directorio_pdfs("pdfs_ensayos/", output_dir="reportes")
```

### OpciÃ³n 2: Evaluar archivos de texto

```bash
python main.py
```

### OpciÃ³n 3: EvaluaciÃ³n masiva de archivos .txt

```bash
python evaluar_batch.py
```

### OpciÃ³n 4: Procesar PDFs sin evaluar (solo limpieza)

```bash
python pdf_processor.py
```

### Uso programÃ¡tico bÃ¡sico:

```python
from agent import EvaluadorEnsayos

# Crear el evaluador
evaluador = EvaluadorEnsayos()

# Evaluar un ensayo
ensayo = """
Tu ensayo aquÃ­...
"""

evaluacion = evaluador.evaluar(ensayo)

# Acceder a los resultados
print(f"PuntuaciÃ³n total: {evaluacion.puntuacion_total}/5.00")
print(f"Calidad tÃ©cnica: {evaluacion.calidad_tecnica.calificacion}/5")
print(f"Comentario: {evaluacion.comentario_general}")
```

### Procesamiento de PDFs (solo extracciÃ³n y limpieza):

```python
from pdf_processor import PDFProcessor

processor = PDFProcessor()

# Procesar un PDF individual
texto_limpio = processor.procesar_pdf(
    "ensayo.pdf",
    output_path="ensayo_limpio.txt",
    limpiar=True  # Usa LLM para limpiar el texto
)

# Procesar directorio completo
textos = processor.procesar_directorio(
    "pdfs/",
    output_dir="textos_limpios/",
    limpiar=True
)
```

## ğŸ“Š Resultados

El sistema genera:

1. **Texto limpio** (si se procesa desde PDF): Ensayo sin nÃºmeros de pÃ¡gina, lÃ­neas cortadas arregladas
2. **Reporte en consola**: EvaluaciÃ³n completa con todas las calificaciones y comentarios
3. **Reporte HTML**: Documento visualmente atractivo con toda la evaluaciÃ³n
4. **Objeto Python**: `EvaluacionEnsayo` con todos los datos estructurados para anÃ¡lisis posterior

### Ejemplo de salida:

```
ğŸ“ 1. CALIDAD TÃ‰CNICA Y RIGOR ACADÃ‰MICO (20%)
   CalificaciÃ³n: 4/5
   El ensayo presenta una estructura coherente y argumentos bien sustentados...

ğŸ¨ 2. CREATIVIDAD Y ORIGINALIDAD (20%)
   CalificaciÃ³n: 5/5
   Destacable propuesta de "tecnologÃ­a educativa comunitaria"...

ğŸ¯ PUNTUACIÃ“N TOTAL PONDERADA: 4.35/5.00
```

## ğŸ”§ PersonalizaciÃ³n

### Cambiar modelo de IA:

```python
# Para evaluaciÃ³n
evaluador = EvaluadorEnsayos(
    model_name="gpt-4o-mini",  # MÃ¡s econÃ³mico
    temperature=0.3
)

# Para limpieza de PDFs
processor = PDFProcessor(
    model_name="gpt-4o-mini",  # Suficiente para limpieza
    temperature=0.1  # Baja para mantener fidelidad
)
```

### Modificar prompts:

Edita `prompts.py` para ajustar los criterios de evaluaciÃ³n o el tono de los comentarios.

### Ajustar ponderaciones:

Modifica el mÃ©todo `calcular_puntuacion_total()` en `models.py`.

### Elegir mÃ©todo de extracciÃ³n de PDF:

```python
# AutomÃ¡tico (prefiere pdfplumber)
processor.procesar_pdf("ensayo.pdf", metodo="auto")

# EspecÃ­fico
processor.procesar_pdf("ensayo.pdf", metodo="pypdf")  # MÃ¡s rÃ¡pido
processor.procesar_pdf("ensayo.pdf", metodo="pdfplumber")  # Mejor calidad
```

## ğŸ› ï¸ TecnologÃ­as

- **LangChain**: Framework para aplicaciones con LLMs
- **LangGraph**: OrquestaciÃ³n de flujos complejos con grafos
- **OpenAI GPT-4**: Modelo de lenguaje para evaluaciÃ³n
- **Pydantic**: ValidaciÃ³n de datos y modelos estructurados
- **pypdf / pdfplumber**: ExtracciÃ³n de texto desde PDFs
- **Python 3.8+**

## ğŸ¯ Casos de Uso

1. **EvaluaciÃ³n de convocatorias**: Procesa y evalÃºa mÃºltiples ensayos enviados en PDF
2. **Feedback automÃ¡tico**: Proporciona retroalimentaciÃ³n detallada a estudiantes
3. **Pre-selecciÃ³n**: Filtra ensayos por puntuaciÃ³n antes de revisiÃ³n humana
4. **Limpieza de documentos**: Procesa PDFs acadÃ©micos para anÃ¡lisis posterior
5. **AnÃ¡lisis comparativo**: Genera estadÃ­sticas de mÃºltiples ensayos

## ğŸ“ Notas Importantes

- El agente estÃ¡ optimizado para ensayos en espaÃ±ol
- Cada evaluaciÃ³n toma aproximadamente 1-2 minutos dependiendo del largo
- **EvaluaciÃ³n**: Se recomienda GPT-4 o GPT-4o para mejores resultados
- **Limpieza de PDF**: GPT-4o-mini es suficiente y mÃ¡s econÃ³mico
- Los comentarios son constructivos y orientados a la mejora
- La limpieza de PDF mantiene TODO el contenido original, solo mejora el formato
- Usa structured output para garantizar calificaciones precisas (1-5)

## ğŸ” Variables de Entorno Requeridas

```env
OPENAI_API_KEY=sk-...  # Tu clave API de OpenAI
```

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:
1. Haz fork del proyecto
2. Crea una rama para tu feature
3. Haz commit de tus cambios
4. Abre un Pull Request

## ğŸ“§ Contacto

Para preguntas o sugerencias, abre un issue en el repositorio.
